{
    "action_l2_regularization":	0.01,
    "activ":	"relu",
    "actor_lr":	0.001,
    "actor_weight_decay":	0.0,
    "agent_name":	"coda_envst-disentangledpush_algddpg_herfutureactual_2_2_relab-push_heuristic_seed0_tb-CODA_max_c-0.75_coda_-250_coda_s-2000_batch2000_optim2",
    "alg":	"ddpg",
    "annotationdict":	{
        "action_l2_regularization":	"l2 penalty for action norm",
        "action_noise":	"maximum std of action noise",
        "activ":	"activation to use for hidden layers in networks",
        "actor_lr":	"actor learning rate",
        "actor_weight_decay":	"weight decay to apply to actor",
        "batch_size":	"batch size for training the actors/critics",
        "clip_target_range":	"q/value targets are clipped to this range",
        "critic_lr":	"critic learning rate",
        "critic_weight_decay":	"weight decay to apply to critic",
        "curiosity_beta":	"beta to use for curiosity_alpha module",
        "device":	"torch device (cpu or gpu)",
        "dg_score_multiplier":	"if using instrinsic goals, score multiplier for goal candidates that are in DG distribution",
        "direct_overshoots":	"if using overshooting, should it be directed in a straight line?",
        "eexplore":	"how often to do completely random exploration (overrides action noise)",
        "entropy_coef":	"Entropy regularization coefficient for SAC",
        "future_warm_up":	"minimum steps in replay buffer needed to stop doing ONLY future sampling",
        "gamma":	"discount factor",
        "go_eexplore":	"epsilon exploration bonus from each point of go explore, when using intrinsic curiosity",
        "go_reset_percent":	"probability to reset episode early for each point of go explore, when using intrinsic curiosity",
        "goal_modalities":	"keys the agent accesses in dictionary env for goals",
        "grad_norm_clipping":	"gradient norm clipping",
        "grad_value_clipping":	"gradient value clipping",
        "her":	"strategy to use for hindsight experience replay",
        "initial_cutoff":	"initial (and minimum) cutoff for intrinsic goal curiosity",
        "initial_explore":	"steps that actor acts randomly for at beginning of training",
        "log_every":	"how often to log things",
        "modalities":	"keys the agent accesses in dictionary env for observations",
        "n_step_returns":	"if using n-step returns, how many steps?",
        "num_envs":	"number of parallel envs to run",
        "num_eval_envs":	"number of parallel eval envs to run",
        "optimize_every":	"how often optimize is called, in terms of environment steps",
        "overshoot_goal_percent":	"if using instrinsic FIRST VISIT goals, should goal be overshot on success?",
        "policy_opt_noise":	"how much policy noise to add to actor optimization",
        "prioritized_mode":	"buffer prioritization strategy",
        "replay_size":	"maximum size of replay buffer",
        "save_replay_buf":	"save replay buffer checkpoint during training?",
        "seed":	"random seed",
        "sigma_l2_regularization":	"l2 regularization on sigma critics log variance",
        "slot_based_state":	"if state is organized by slot; i.e., [batch_size, num_slots, slot_feats]",
        "sparse_reward_shaping":	"coefficient of euclidean distance reward shaping in sparse goal envs",
        "target_network_update_frac":	"polyak averaging coefficient for target networks",
        "target_network_update_freq":	"how often to update target networks; NOTE: TD3 uses this too!",
        "td3_delay":	"how often the actor is trained, in terms of critic training steps, in td3",
        "td3_noise":	"noise added to next step actions in td3",
        "td3_noise_clip":	"amount to which next step noise in td3 is clipped",
        "use_actor_target":	"if true, use actor target network to act in the environment",
        "varied_action_noise":	"if true, action noise for each env in vecenv is interpolated between 0 and action noise",
        "warm_up":	"minimum steps in replay buffer needed to optimize"
    },
    "batch_size":	2000,
    "clip_target_range":	[
        -50.0,
        0.0
    ],
    "critic_lr":	0.001,
    "critic_weight_decay":	0.0,
    "curiosity_beta":	-3.0,
    "cutoff_success_threshold":	[
        0.3,
        0.7
    ],
    "device":	"cuda",
    "dg_score_multiplier":	1.0,
    "direct_overshoots":	false,
    "eexplore":	0.2,
    "entropy_coef":	0.2,
    "env_steps":	0,
    "future_warm_up":	25000,
    "gamma":	0.98,
    "go_eexplore":	0.1,
    "go_reset_percent":	0.0,
    "goal_modalities":	[
        "desired_goal"
    ],
    "grad_norm_clipping":	-1.0,
    "grad_value_clipping":	-1,
    "her":	"futureactual_2_2",
    "initial_cutoff":	-3,
    "initial_explore":	10000,
    "log_every":	5000,
    "min_experience_to_train_coda_attn":	25000,
    "modalities":	[
        "observation"
    ],
    "module_action_noise":	"{'self': <mrl.modules.action_noise.ContinuousActionNoise object at 0x7ff098a927d0>, 'random_process_cls': <class 'mrl.utils.random_process.GaussianProcess'>, 'args': (), 'kwargs': {'std': <mrl.utils.schedule.ConstantSchedule object at 0x7ff098a92950>}}",
    "module_actor":	"{'self': <mrl.modules.model.PytorchModel object at 0x7ff098a87890>, 'name': 'actor', 'model_fn': <function main.<locals>.<lambda> at 0x7ff0913a63b0>}",
    "module_algorithm":	"{'self': <mrl.algorithms.continuous_off_policy.DDPG object at 0x7ff098a8bfd0>}",
    "module_critic":	"{'self': <mrl.modules.model.PytorchModel object at 0x7ff0913b6750>, 'name': 'critic', 'model_fn': <function main.<locals>.<lambda> at 0x7ff09125c830>}",
    "module_env":	"{'self': <mrl.modules.env.EnvModule object at 0x7ff098a87bd0>, 'num_envs': 6, 'name': None, 'modalities': ['observation'], 'goal_modalities': ['desired_goal'], 'env': <function make_disentangled_fetch_env.<locals>.env at 0x7ff098a7b200>, 'episode_life': True, 'seed': 0}",
    "module_eval":	"{'self': <mrl.modules.eval.EpisodicEval object at 0x7ff098a92810>}",
    "module_eval_env":	"{'self': <mrl.modules.env.EnvModule object at 0x7ff098a87850>, 'num_envs': 6, 'name': 'eval_env', 'modalities': ['observation'], 'goal_modalities': ['desired_goal'], 'env': <function make_disentangled_fetch_env.<locals>.env at 0x7ff098a7b200>, 'episode_life': True, 'seed': 1138}",
    "module_logger":	"{'self': <mrl.modules.logging.Logger object at 0x7ff098a92910>, 'average_every': 100}",
    "module_policy":	"{'self': <mrl.algorithms.continuous_off_policy.ActorPolicy object at 0x7ff098a92990>}",
    "module_replay_buffer":	"{'self': <experiments.coda.coda_module.CodaBuffer object at 0x7ff0913b6b10>, 'module_name': 'replay_buffer'}",
    "module_state_normalizer":	"{'self': <mrl.modules.normalizer.Normalizer object at 0x7ff098a929d0>, 'normalizer': <mrl.modules.normalizer.MeanStdNormalizer object at 0x7ff098a92890>}",
    "module_train":	"{'self': <mrl.modules.train.StandardTrain object at 0x7ff09ad97cd0>}",
    "n_step_returns":	1,
    "never_done":	true,
    "num_envs":	6,
    "num_eval_envs":	6,
    "opt_steps":	0,
    "optimize_every":	2,
    "other_args":	{
        "add_bad_dcs":	false,
        "attn_mech_dir":	null,
        "checkpoint_dir":	null,
        "coda_buffer_size":	3000000,
        "coda_every":	250,
        "coda_on_goal_components":	false,
        "coda_samples_per_pair":	2,
        "coda_source_pairs":	2000,
        "envstr":	"disentangledpush",
        "epoch_len":	5000,
        "launch_command":	"/home/zhoujie/mrl/experiments/coda/train_coda_copy.py ",
        "layers":	[
            512,
            512,
            512
        ],
        "max_coda_ratio":	0.75,
        "max_steps":	1000000,
        "min_experience_to_train_coda_attn":	25000,
        "prefix":	"coda",
        "relabel_type":	"push_heuristic",
        "save_embeddings":	false,
        "tb":	"CODA",
        "thresh":	0.02,
        "train_dt":	0.05,
        "visualize_trained_agent":	false
    },
    "overshoot_goal_percent":	0.0,
    "parent_folder":	"./push_results",
    "policy_opt_noise":	0.0,
    "prioritized_mode":	"none",
    "replay_size":	2500000,
    "save_replay_buf":	false,
    "seed":	0,
    "sigma_l2_regularization":	0.0,
    "slot_action_dims":	null,
    "slot_based_state":	false,
    "slot_state_dims":	[
        "[0 1 2 3 4 5 6 7 8 9]",
        "[10 11 12 13 14 15 16 17 18 19 20 21]"
    ],
    "sparse_reward_shaping":	0.0,
    "target_network_update_frac":	0.05,
    "target_network_update_freq":	10,
    "td3_delay":	2,
    "td3_noise":	0.1,
    "td3_noise_clip":	0.3,
    "use_actor_target":	false,
    "varied_action_noise":	false,
    "warm_up":	5000
}